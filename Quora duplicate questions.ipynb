{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    63.08\n",
       "1    36.92\n",
       "Name: is_duplicate, dtype: float64"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Set environment\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('precision', 3)\n",
    "# Read data\n",
    "df_o=pd.read_csv('./data/train.csv',dtype={'question1':str,'question2':str})\n",
    "\n",
    "\n",
    "#Inspect data\n",
    "df_o.head()\n",
    "df_o['is_duplicate'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               int64\n",
       "qid1             int64\n",
       "qid2             int64\n",
       "question1       object\n",
       "question2       object\n",
       "is_duplicate     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_o.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_o.drop_duplicates(inplace=True)\n",
    "df_o.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404288, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset in training and validation dataset, 70/30\n",
    "# Test dataset provided will not be used until the end, so it can be used for final validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "TEST_SIZE = 0.3\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_o.ix[:,0:4], df_o.ix[:,'is_duplicate'], test_size=TEST_SIZE, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate features\n",
    "\n",
    "Some ideas to work on:\n",
    "1. Question lenght in words\n",
    "2. Shared words\n",
    "3. Use tf-idf to identify specific words on each pairs\n",
    "4. Is question clause the same?\n",
    "5. Syntax similarity: POS tags?\n",
    "6. Shared synonims (all words or just the key words- verb, object)\n",
    "7. Similarity of key words: wordnet/synnet similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up functions\n",
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def clean_text(s):\n",
    "    # call with a string and return a list of words excluding stop words and punctuation and lower case\n",
    "    \n",
    "    tok=tokenize.word_tokenize(s)\n",
    "    s1=[w.lower() for w in tok if w not in stopwords.words('english')]\n",
    "    \n",
    "    punct=set(string.punctuation)\n",
    "    s2=[w for w in s1 if w not in punct]\n",
    "    return s2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: different length in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dif_len(l1,l2):\n",
    "    return(abs(len(l1.split())-len(l2.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Feature: Number of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Use tf-idf with cosine similarity\n",
    "The approach is to identify \"vectorize\" the strings using tf idf and then find the similarity between the two.\n",
    "This feature will produce a number between the 2 sentences that will represent how close they are between each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to determine the tf idf of a list of strings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf(l1, l2):\n",
    "    # \n",
    "    result=[]\n",
    "    for X in zip(l1,l2):\n",
    "        tfidf_matrix = []\n",
    "        tfidf_vect=TfidfVectorizer()\n",
    "        tfidf_sparse=tfidf_vect.fit_transform(X)\n",
    "        tfidf_matrix=tfidf_sparse.todense()\n",
    "        #print (tfidf_matrix,\"*****\")\n",
    "        cosine_distance=cos_sim(tfidf_matrix)\n",
    "        result.append(cosine_distance)\n",
    "    return result\n",
    "\n",
    "# function to identify the cosine similarity between 2 vectors\n",
    "import sklearn.metrics.pairwise as metrics\n",
    "def cos_sim(m):\n",
    "    cos=metrics.cosine_similarity(m[0],m[1])[0,0]\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect=TfidfVectorizer(stop_words='english')\n",
    "tfidf_list=tfidf_vect.fit_transform(X_train['question1']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_list=X_train['question1'].tolist()\n",
    "q2_list=X_train['question2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cos_dist=tfidf(q1_list[0:100],q2_list[0:100])\n",
    "Y_100=y_train[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.23577033983032974, 0),\n",
       " (0.91381065092944613, 1),\n",
       " (0.20469348972795967, 0),\n",
       " (0.79586433687836444, 0),\n",
       " (0.29121941856368966, 1),\n",
       " (0.31169551191229922, 1),\n",
       " (0.57273935841961987, 1),\n",
       " (0.063722333236223103, 0),\n",
       " (0.6694188517266485, 1),\n",
       " (0.0, 0),\n",
       " (0.14211771769343498, 0),\n",
       " (0.22185010606887803, 0),\n",
       " (0.42113589132819579, 0),\n",
       " (0.89010872503406635, 0),\n",
       " (0.75489691266928305, 0),\n",
       " (0.10112251282501054, 0),\n",
       " (0.33609692727625745, 0),\n",
       " (0.18976728433844914, 0),\n",
       " (0.75489691266928305, 1),\n",
       " (0.0, 0),\n",
       " (0.88312820391945235, 0),\n",
       " (0.15906444589068597, 0),\n",
       " (0.33609692727625756, 1),\n",
       " (0.34861427265775857, 1),\n",
       " (0.20064995690540438, 1),\n",
       " (0.71681174144306237, 1),\n",
       " (0.30392422712517309, 0),\n",
       " (0.18516298920548105, 0),\n",
       " (0.26725230169548991, 1),\n",
       " (0.15064018498706505, 0),\n",
       " (0.88363513889950873, 1),\n",
       " (0.15186371361376427, 0),\n",
       " (0.27140359420048249, 0),\n",
       " (0.51509784183590801, 0),\n",
       " (0.50560555887396907, 1),\n",
       " (0.33609692727625745, 0),\n",
       " (0.93822324785867173, 1),\n",
       " (0.89553241507157266, 1),\n",
       " (0.56983445370680674, 1),\n",
       " (0.53320876075326329, 1),\n",
       " (0.058509704133672943, 0),\n",
       " (0.64732863375138805, 1),\n",
       " (0.28196020042685588, 1),\n",
       " (0.60832555636579966, 1),\n",
       " (0.20613696606828608, 0),\n",
       " (0.1908598900456927, 0),\n",
       " (0.34464214103805479, 1),\n",
       " (0.27519581217502298, 0),\n",
       " (0.40905205992283844, 1),\n",
       " (0.40298220897396103, 0),\n",
       " (0.96476382123773219, 1),\n",
       " (0.32296297949918207, 0),\n",
       " (0.28530619098136084, 1),\n",
       " (0.43330094650894713, 1),\n",
       " (0.08607577920972273, 0),\n",
       " (0.53320876075326329, 1),\n",
       " (0.5586177528223194, 1),\n",
       " (0.043385164728150641, 0),\n",
       " (0.15064018498706511, 0),\n",
       " (0.070984137347014142, 0),\n",
       " (0.10658141524382435, 0),\n",
       " (0.63923062405362863, 1),\n",
       " (0.86736368496092875, 1),\n",
       " (0.12735952979479356, 0),\n",
       " (0.68843645482349602, 0),\n",
       " (0.18832393622758895, 0),\n",
       " (0.12389761953733384, 0),\n",
       " (0.50310261241513143, 1),\n",
       " (0.33609692727625745, 0),\n",
       " (0.32943210596310879, 0),\n",
       " (0.53192527495433761, 1),\n",
       " (0.474330706497194, 1),\n",
       " (0.63279045836799463, 0),\n",
       " (0.2061369660682861, 0),\n",
       " (0.22576484600261607, 0),\n",
       " (0.45026814465562653, 0),\n",
       " (0.8035535489054062, 1),\n",
       " (0.7092972666062739, 1),\n",
       " (0.66571417726372639, 1),\n",
       " (0.073574746105895109, 0),\n",
       " (0.40840369224375006, 1),\n",
       " (0.67352050457808121, 1),\n",
       " (0.40739778621409745, 0),\n",
       " (1.0000000000000002, 0),\n",
       " (0.1350734836708713, 0),\n",
       " (0.14978520952711727, 0),\n",
       " (0.081768041465999378, 0),\n",
       " (0.20380370846518475, 0),\n",
       " (0.63279045836799463, 1),\n",
       " (0.56144163005842085, 0),\n",
       " (0.58033298467656858, 1),\n",
       " (0.25233420143369617, 0),\n",
       " (0.42113589132819579, 0),\n",
       " (0.19261467065293905, 0),\n",
       " (0.77991542455799756, 1),\n",
       " (0.034287183990724272, 0),\n",
       " (0.1350734836708713, 0),\n",
       " (0.74629748619384284, 0),\n",
       " (0.26969665998264064, 1),\n",
       " (0.72410726960293625, 0)]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cos_dist,Y_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Shared synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature: Similarity of key words: noun and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add feature columns to train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the story of Kohinoor (Koh-i-Noor) Diamond?'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.ix[1,'question1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will take LogisticRegression as a simple algorithm to establish a baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flow: \n",
    "#   train model with a set of hyperparameters\n",
    "#   Obtain score and iterate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
